{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0534a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongodb_lib import *\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634cdb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully connected to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "config_infra = yaml.load(open(\"../infra-config-pipeline.yaml\"), Loader=yaml.FullLoader)\n",
    "db, fs, client = connect_to_mongodb(config_infra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "311e000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import taxonomy\n",
    "\n",
    "taxonomy = pd.read_excel(\"../Categories.xlsx\")\n",
    "taxonomy = taxonomy[[\"Sub-category\", \"Description & Keywords.1\"]]\n",
    "\n",
    "taxonomy_json = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for key, value in zip(taxonomy[\"Sub-category\"], taxonomy[\"Description & Keywords.1\"]):\n",
    "\n",
    "    entries = value.split(\"\\n\")\n",
    "    assert len(entries) == 2\n",
    "\n",
    "    taxonomy_json[key][\"description\"] = entries[0].replace(\"- Description: \",\"\")\n",
    "    taxonomy_json[key][\"keywords\"] = entries[1].replace(\"- Keywords: \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2428366",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_object(fs, \"product_textual_lang_summarized\")\n",
    "data = pd.DataFrame(data)\n",
    "data.fillna(\"\", inplace=True)\n",
    "data = data[[\"pdt_product_detail_PRODUCTDESCRIPTION_SUMMARIZED\"]]\n",
    "data.columns = [\"description\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f193dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a018f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/Desktop/data-exploration/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # You can choose other variants as well\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f632937",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tokenize and encode texts\n",
    "encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87804ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # You can choose other variants as well\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_inputs)\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Convert taxonomy keywords to embeddings (average of word embeddings)\n",
    "taxonomy_embeddings = {}\n",
    "for category, info in taxonomy.items():\n",
    "    keywords = info['keywords']\n",
    "    keyword_embeddings = []\n",
    "    for keyword in keywords.split(', '):\n",
    "        token_ids = tokenizer.encode(keyword, add_special_tokens=False, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            keyword_output = model(token_ids)[0]\n",
    "        keyword_embedding = torch.mean(keyword_output, dim=1)\n",
    "        keyword_embeddings.append(keyword_embedding)\n",
    "    taxonomy_embeddings[category] = torch.mean(torch.cat(keyword_embeddings, dim=0), dim=0).numpy()\n",
    "\n",
    "# Compute cosine similarity between each text embedding and each taxonomy category embedding\n",
    "similarities = cosine_similarity(embeddings.numpy(), np.array(list(taxonomy_embeddings.values())))\n",
    "\n",
    "# Threshold for classification\n",
    "threshold = 0.5\n",
    "\n",
    "# Classify each text\n",
    "classified_categories = []\n",
    "for i, text in enumerate(texts):\n",
    "    categories = []\n",
    "    for j, (category, _) in enumerate(taxonomy.items()):\n",
    "        if similarities[i][j] > threshold:\n",
    "            categories.append(category)\n",
    "    classified_categories.append(categories)\n",
    "\n",
    "# Add classified categories to DataFrame\n",
    "data['classified_categories'] = classified_categories\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
